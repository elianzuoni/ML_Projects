{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_raw, X_raw, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 datapoints, of which 181886 have at least one missing feature\n",
      "Fraction of null datapoints for each feature:\n",
      " [0.152456 0.       0.       0.       0.709828 0.709828 0.709828 0.\n",
      " 0.       0.       0.       0.       0.709828 0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.399652\n",
      " 0.399652 0.399652 0.709828 0.709828 0.709828 0.      ]\n"
     ]
    }
   ],
   "source": [
    "N = X_raw.shape[0]\n",
    "D = X_raw.shape[1]\n",
    "null = -999\n",
    "\n",
    "# No feature dropping yet\n",
    "\n",
    "# Total number of rows for which there is at least one null value\n",
    "points_missing = np.sum(np.any(X_raw==null, axis=1))\n",
    "# Vector holding, for each feature, the fraction of datapoints with a null in that position\n",
    "null_frac = (1/N) * np.sum(X_raw==null, axis=0)\n",
    "\n",
    "print(N, \"datapoints, of which\", points_missing, \"have at least one missing feature\")\n",
    "print(\"Fraction of null datapoints for each feature:\\n\", null_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (250000, 20) , has nulls: False\n"
     ]
    }
   ],
   "source": [
    "from toolbox.manipulate_data import *\n",
    "\n",
    "# There are only 3 different values in null_frac\n",
    "drop_thresholds = [0, 0.33, 0.7, 1]\n",
    "# Specific choice\n",
    "drop_thresh = drop_thresholds[1]\n",
    "\n",
    "# Clean dataset\n",
    "X = clean_data(X_raw, null, drop_thresh)\n",
    "\n",
    "#Verify that there are no missing values anymore\n",
    "print(\"X.shape:\", X.shape, \", has nulls:\", np.any(X == null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) (20,)\n"
     ]
    }
   ],
   "source": [
    "# Standardise dataset\n",
    "X, mean_X, std_X = standardise(X)\n",
    "print(mean_X.shape, std_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map y to {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Map y from bilateral {-1, +1} domain to unilateral {0, 1} domain\n",
    "print(y_raw)\n",
    "y = bin_bil_to_unil(y_raw)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 21)\n"
     ]
    }
   ],
   "source": [
    "# Expand features\n",
    "degree = 1\n",
    "tX = expand_features(X, degree)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different training functions and hyperparameters with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularised Least-Squares with NE\n",
      "log10_lambda = -10.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -9.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -8.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -7.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -6.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -5.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -4.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -3.000: avg_test_loss = 0.266, avg_train_loss = 0.088\n",
      "log10_lambda = -2.000: avg_test_loss = 0.269, avg_train_loss = 0.090\n",
      "log10_lambda = -1.000: avg_test_loss = 0.299, avg_train_loss = 0.103\n",
      "\n",
      "Regularised Logistic with GD\n",
      "log10_lambda = -10.000: avg_test_loss = 0.287, avg_train_loss = 103109.257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-643ce07c2a8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mhyper_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_log_GD, hyper_params, \n\u001b[0m\u001b[0;32m     28\u001b[0m                                                      class_thresh, k_fold, \"classifier\")\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\testing.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, tx, trainer, hyper_params, threshold, k_fold, to_assess)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hyper_params is a tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Get test loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\u001b[0m in \u001b[0;36mtrain_reg_log_GD\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma, threshold)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;34m\"\"\" Training for L2-regularised Logistic loss with Gradient Descent \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mlearn_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_linear_regressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(learn_args, learn, get_regressor, threshold)\u001b[0m\n\u001b[0;32m     36\u001b[0m     Returns the model (w, loss) and the corresponding regressor and classifier. \"\"\"\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Learn the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearn_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Get the regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;34m\"\"\" Gradient Descent for the L2-regularised logistic cost function. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m# Just provide the right loss and gradient functions to the generic implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_reg_logistic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_reg_logistic_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(y, tx, initial_w, max_iters, gamma, lambda_, compute_loss, compute_gradient)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# Update w, and the corresponding loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mcompute_reg_logistic_loss\u001b[1;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Vectorised computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Regularise, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from toolbox.training import *\n",
    "from toolbox.testing import *\n",
    "\n",
    "lambdas = np.logspace(-10, -1, 10)\n",
    "max_iters = 10000\n",
    "gamma = 1e-5\n",
    "class_thresh = 0.5\n",
    "k_fold = 5\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "\n",
    "# Regularised Least-Squares with NE\n",
    "print(\"Regularised Least-Squares with NE\")\n",
    "for lambda_ in lambdas:\n",
    "    hyper_params = (lambda_, )\n",
    "    avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_ls_NE, hyper_params, \n",
    "                                                     class_thresh, k_fold, \"classifier\")\n",
    "\n",
    "    print(\"log10_lambda = {log10_lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "    log10_lam=np.log10(lambda_), avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "\n",
    "\n",
    "# Regularised Logistic with GD\n",
    "print(\"\\nRegularised Logistic with GD\")\n",
    "for lambda_ in lambdas:\n",
    "    hyper_params = (lambda_, initial_w, max_iters, gamma)\n",
    "    avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_log_GD, hyper_params, \n",
    "                                                     class_thresh, k_fold, \"classifier\")\n",
    "\n",
    "    print(\"log10_lambda = {log10_lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "    log10_lam=np.log10(lambda_), avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using best method and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08812559481524469\n"
     ]
    }
   ],
   "source": [
    "w, train_loss, regressor, classifier = train_unreg_ls_NE(y, tX, class_thresh)\n",
    "\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load challenge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_ch_raw, ids_ch = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, standardise, and expand challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "X_ch = clean_data(X_ch_raw, null, drop_thresh)\n",
    "\n",
    "# Standardise dataset\n",
    "X_ch, mean_X_ch, std_X_ch = standardise(X_ch)\n",
    "\n",
    "# Expand features\n",
    "tX_ch = expand_features(X_ch, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n",
      "[-1 -1 -1 ...  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Get output in {0, 1}\n",
    "y_unil = classifier(tX_ch)\n",
    "\n",
    "# Map output to {-1, +1}\n",
    "y_bil = bin_unil_to_bil(y_unil)\n",
    "\n",
    "print(y_unil)\n",
    "print(y_bil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write formatted output file\n",
    "OUTPUT_PATH = 'submission.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_ch, y_bil, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
