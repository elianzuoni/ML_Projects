{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_raw, X_raw, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 datapoints, of which 181886 have at least one missing feature\n",
      "Fraction of null datapoints for each feature:\n",
      " [0.152456 0.       0.       0.       0.709828 0.709828 0.709828 0.\n",
      " 0.       0.       0.       0.       0.709828 0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.399652\n",
      " 0.399652 0.399652 0.709828 0.709828 0.709828 0.      ]\n"
     ]
    }
   ],
   "source": [
    "N = X_raw.shape[0]\n",
    "D = X_raw.shape[1]\n",
    "null = -999\n",
    "\n",
    "# No feature dropping yet\n",
    "\n",
    "# Total number of rows for which there is at least one null value\n",
    "points_missing = np.sum(np.any(X_raw==null, axis=1))\n",
    "# Vector holding, for each feature, the fraction of datapoints with a null in that position\n",
    "null_frac = (1/N) * np.sum(X_raw==null, axis=0)\n",
    "\n",
    "print(N, \"datapoints, of which\", points_missing, \"have at least one missing feature\")\n",
    "print(\"Fraction of null datapoints for each feature:\\n\", null_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (250000, 23) , has nulls: False\n"
     ]
    }
   ],
   "source": [
    "from toolbox.manipulate_data import *\n",
    "\n",
    "# There are only 4 different values in null_frac\n",
    "drop_thresholds = [0.1, 0.33, 0.7, 1]\n",
    "# Specific choice\n",
    "drop_thresh = drop_thresholds[2]\n",
    "\n",
    "# Clean dataset\n",
    "X = clean_data(X_raw, null, drop_thresh)\n",
    "\n",
    "#Verify that there are no missing values anymore\n",
    "print(\"X.shape:\", X.shape, \", has nulls:\", np.any(X == null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,) (23,)\n"
     ]
    }
   ],
   "source": [
    "# Standardise dataset\n",
    "X, mean_X, std_X = standardise(X)\n",
    "print(mean_X.shape, std_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map y to {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Map y from bilateral {-1, +1} domain to unilateral {0, 1} domain\n",
    "print(y_raw)\n",
    "y = bin_bil_to_unil(y_raw)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 231)\n"
     ]
    }
   ],
   "source": [
    "# Expand features\n",
    "degree = 10\n",
    "tX = expand_features(X, degree)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different training functions and hyperparameters with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularised Least-Squares with NE\n",
      "log10_lambda = -10.000: avg_test_loss = 0.196, avg_train_loss = 0.163\n",
      "log10_lambda = -9.500: avg_test_loss = 0.190, avg_train_loss = 0.079\n",
      "log10_lambda = -9.000: avg_test_loss = 0.189, avg_train_loss = 0.074\n",
      "log10_lambda = -8.500: avg_test_loss = 0.190, avg_train_loss = 0.074\n",
      "log10_lambda = -8.000: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -7.500: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -7.000: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -6.500: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -6.000: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -5.500: avg_test_loss = 0.190, avg_train_loss = 0.071\n",
      "log10_lambda = -5.000: avg_test_loss = 0.215, avg_train_loss = 0.118\n",
      "log10_lambda = -4.500: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -4.000: avg_test_loss = 0.189, avg_train_loss = 0.071\n",
      "log10_lambda = -3.500: avg_test_loss = 0.190, avg_train_loss = 0.071\n",
      "log10_lambda = -3.000: avg_test_loss = 0.190, avg_train_loss = 0.071\n",
      "log10_lambda = -2.500: avg_test_loss = 0.190, avg_train_loss = 0.072\n",
      "log10_lambda = -2.000: avg_test_loss = 0.193, avg_train_loss = 0.073\n",
      "log10_lambda = -1.500: avg_test_loss = 0.201, avg_train_loss = 0.076\n",
      "log10_lambda = -1.000: avg_test_loss = 0.218, avg_train_loss = 0.080\n",
      "log10_lambda = -0.500: avg_test_loss = 0.242, avg_train_loss = 0.088\n",
      "log10_lambda = 0.000: avg_test_loss = 0.266, avg_train_loss = 0.094\n",
      "log10_lambda = 0.500: avg_test_loss = 0.288, avg_train_loss = 0.100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fcf985a4e143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mhyper_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_ls_NE, hyper_params, \n\u001b[0m\u001b[0;32m     18\u001b[0m                                                      class_thresh, k_fold, \"classifier\")\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\testing.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, tx, trainer, hyper_params, threshold, k_fold, to_assess)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# Concatenate along rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mtx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_blocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_blocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# Remove first row (initialisation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from toolbox.training import *\n",
    "from toolbox.testing import *\n",
    "\n",
    "lambdas = np.logspace(-10, 2, 25)\n",
    "max_iters = 100\n",
    "batch_size = 200\n",
    "gamma = 1e-6\n",
    "class_thresh = 0.5\n",
    "k_fold = 5\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "\n",
    "# Regularised Least-Squares with NE\n",
    "print(\"Regularised Least-Squares with NE\")\n",
    "for lambda_ in lambdas:\n",
    "    hyper_params = (lambda_, )\n",
    "    avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_ls_NE, hyper_params, \n",
    "                                                     class_thresh, k_fold, \"classifier\")\n",
    "\n",
    "    print(\"log10_lambda = {log10_lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "    log10_lam=np.log10(lambda_), avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "\n",
    "\n",
    "# Regularised Logistic with GD\n",
    "print(\"\\nRegularised Logistic with SGD\")\n",
    "for lambda_ in lambdas:\n",
    "    hyper_params = (lambda_, initial_w, max_iters, batch_size, gamma)\n",
    "    avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_log_SGD, hyper_params, \n",
    "                                                     class_thresh, k_fold, \"classifier\")\n",
    "\n",
    "    print(\"log10_lambda = {log10_lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "    log10_lam=np.log10(lambda_), avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using best method and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08812559481524469\n"
     ]
    }
   ],
   "source": [
    "w, train_loss, regressor, classifier = train_unreg_ls_NE(y, tX, class_thresh)\n",
    "\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load challenge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_ch_raw, ids_ch = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, standardise, and expand challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "X_ch = clean_data(X_ch_raw, null, drop_thresh)\n",
    "\n",
    "# Standardise dataset\n",
    "X_ch, mean_X_ch, std_X_ch = standardise(X_ch)\n",
    "\n",
    "# Expand features\n",
    "tX_ch = expand_features(X_ch, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n",
      "[-1 -1 -1 ...  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Get output in {0, 1}\n",
    "y_unil = classifier(tX_ch)\n",
    "\n",
    "# Map output to {-1, +1}\n",
    "y_bil = bin_unil_to_bil(y_unil)\n",
    "\n",
    "print(y_unil)\n",
    "print(y_bil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write formatted output file\n",
    "OUTPUT_PATH = 'submission.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_ch, y_bil, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Directly use run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file created\n"
     ]
    }
   ],
   "source": [
    "%run run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
