{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_raw, X_raw, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000 datapoints, of which 181886 have at least one missing feature\n",
      "Fraction of null datapoints for each feature:\n",
      " [0.152456 0.       0.       0.       0.709828 0.709828 0.709828 0.\n",
      " 0.       0.       0.       0.       0.709828 0.       0.       0.\n",
      " 0.       0.       0.       0.       0.       0.       0.       0.399652\n",
      " 0.399652 0.399652 0.709828 0.709828 0.709828 0.      ]\n"
     ]
    }
   ],
   "source": [
    "N = X_raw.shape[0]\n",
    "D = X_raw.shape[1]\n",
    "null = -999\n",
    "\n",
    "# No feature dropping yet\n",
    "\n",
    "# Total number of rows for which there is at least one null value\n",
    "points_missing = np.sum(np.any(X_raw==null, axis=1))\n",
    "# Vector holding, for each feature, the fraction of datapoints with a null in that position\n",
    "null_frac = (1/N) * np.sum(X_raw==null, axis=0)\n",
    "\n",
    "print(N, \"datapoints, of which\", points_missing, \"have at least one missing feature\")\n",
    "print(\"Fraction of null datapoints for each feature:\\n\", null_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (250000, 30) , has nulls: False\n"
     ]
    }
   ],
   "source": [
    "from toolbox.manipulate_data import *\n",
    "\n",
    "# There are only 4 different values in null_frac\n",
    "drop_thresholds = [0.1, 0.33, 0.7, 1]\n",
    "# Specific choice\n",
    "drop_thresh = drop_thresholds[3]\n",
    "\n",
    "# Clean dataset\n",
    "X = clean_data(X_raw, null, drop_thresh)\n",
    "\n",
    "#Verify that there are no missing values anymore\n",
    "print(\"X.shape:\", X.shape, \", has nulls:\", np.any(X == null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Standardise dataset\n",
    "X, mean_X, std_X = standardise(X)\n",
    "print(mean_X.shape, std_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map y to {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Map y from bilateral {-1, +1} domain to unilateral {0, 1} domain\n",
    "print(y_raw)\n",
    "y = bin_bil_to_unil(y_raw)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 361)\n"
     ]
    }
   ],
   "source": [
    "# Expand features\n",
    "degree = 12\n",
    "tX = expand_features(X, degree)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different training functions and hyperparameters with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularised Least-Squares with NE\n",
      "log10_lambda = -10.000: avg_test_loss = 0.187, avg_train_loss = 0.536\n",
      "log10_lambda = -9.500: avg_test_loss = 0.183, avg_train_loss = 1.769\n",
      "log10_lambda = -9.000: avg_test_loss = 0.182, avg_train_loss = 0.077\n",
      "log10_lambda = -8.500: avg_test_loss = 0.182, avg_train_loss = 0.084\n",
      "log10_lambda = -8.000: avg_test_loss = 0.183, avg_train_loss = 0.073\n",
      "log10_lambda = -7.500: avg_test_loss = 0.182, avg_train_loss = 0.078\n",
      "log10_lambda = -7.000: avg_test_loss = 0.182, avg_train_loss = 0.068\n",
      "log10_lambda = -6.500: avg_test_loss = 0.184, avg_train_loss = 0.070\n",
      "log10_lambda = -6.000: avg_test_loss = 0.182, avg_train_loss = 0.068\n",
      "log10_lambda = -5.500: avg_test_loss = 0.182, avg_train_loss = 0.068\n",
      "log10_lambda = -5.000: avg_test_loss = 0.185, avg_train_loss = 0.070\n",
      "log10_lambda = -4.500: avg_test_loss = 0.183, avg_train_loss = 0.069\n",
      "log10_lambda = -4.000: avg_test_loss = 0.194, avg_train_loss = 0.076\n",
      "log10_lambda = -3.500: avg_test_loss = 0.183, avg_train_loss = 0.068\n",
      "log10_lambda = -3.000: avg_test_loss = 0.247, avg_train_loss = 0.647\n",
      "log10_lambda = -2.500: avg_test_loss = 0.192, avg_train_loss = 0.079\n",
      "log10_lambda = -2.000: avg_test_loss = 0.186, avg_train_loss = 0.071\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-7b263e5fdc3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mhyper_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_ls_NE, hyper_params, \n\u001b[0m\u001b[0;32m     18\u001b[0m                                                      class_thresh, k_fold, \"classifier\")\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\testing.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, tx, trainer, hyper_params, threshold, k_fold, to_assess)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hyper_params is a tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Get test loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\u001b[0m in \u001b[0;36mtrain_reg_ls_NE\u001b[1;34m(y, tx, lambda_, threshold)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;34m\"\"\" Training for L2-regularised Least-Squares loss with Normal Equations \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mlearn_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_linear_regressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(learn_args, learn, get_regressor, threshold)\u001b[0m\n\u001b[0;32m     36\u001b[0m     Returns the model (w, loss) and the corresponding regressor and classifier. \"\"\"\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Learn the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearn_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Get the regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mridge_regression\u001b[1;34m(y, tx, lambda_)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[0mlambda_prime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;31m# Closed formula for optimal w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_prime\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_reg_ls_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from toolbox.training import *\n",
    "from toolbox.testing import *\n",
    "\n",
    "lambdas = np.logspace(-10, 2, 25)\n",
    "max_iters = 300\n",
    "batch_size = 2000\n",
    "gamma = 1e-11\n",
    "class_thresh = 0.5\n",
    "k_fold = 5\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "\n",
    "# Regularised Least-Squares with NE\n",
    "print(\"Regularised Least-Squares with NE\")\n",
    "for lambda_ in lambdas:\n",
    "    hyper_params = (lambda_, )\n",
    "    avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_ls_NE, hyper_params, \n",
    "                                                     class_thresh, k_fold, \"classifier\")\n",
    "\n",
    "    print(\"log10_lambda = {log10_lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "    log10_lam=np.log10(lambda_), avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "\n",
    "\n",
    "# Regularised Logistic with GD\n",
    "print(\"\\nRegularised Logistic with SGD\")\n",
    "for lambda_ in lambdas:\n",
    "    hyper_params = (lambda_, initial_w, max_iters, batch_size, gamma)\n",
    "    avg_test_loss, avg_train_loss = cross_validation(y, tX, train_reg_log_SGD, hyper_params, \n",
    "                                                     class_thresh, k_fold, \"classifier\")\n",
    "\n",
    "    print(\"log10_lambda = {log10_lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "    log10_lam=np.log10(lambda_), avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using best method and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08812559481524469\n"
     ]
    }
   ],
   "source": [
    "w, train_loss, regressor, classifier = train_unreg_ls_NE(y, tX, class_thresh)\n",
    "\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load challenge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_ch_raw, ids_ch = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, standardise, and expand challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "X_ch = clean_data(X_ch_raw, null, drop_thresh)\n",
    "\n",
    "# Standardise dataset\n",
    "X_ch, mean_X_ch, std_X_ch = standardise(X_ch)\n",
    "\n",
    "# Expand features\n",
    "tX_ch = expand_features(X_ch, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n",
      "[-1 -1 -1 ...  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Get output in {0, 1}\n",
    "y_unil = classifier(tX_ch)\n",
    "\n",
    "# Map output to {-1, +1}\n",
    "y_bil = bin_unil_to_bil(y_unil)\n",
    "\n",
    "print(y_unil)\n",
    "print(y_bil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write formatted output file\n",
    "OUTPUT_PATH = 'submission.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_ch, y_bil, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Directly use run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file created\n"
     ]
    }
   ],
   "source": [
    "%run run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
