{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'C:/Users/samsung/ML_Projects/project1/data/train/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand and clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (250000, 30) ||| Y shape :  (250000,) ||| ids shape :  (250000,)\n",
      "y takes value in :  [-1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Firt let's see the shape of our dataset\n",
    "print('X shape : ', tX.shape, '||| Y shape : ', y.shape, '||| ids shape : ', ids.shape)\n",
    "\n",
    "#Let's see what values take y :\n",
    "print('y takes value in : ',np.unique(y)) #we suppose -1 means that the boson haven't been observed, and 1 mean the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the label of the first entry on our data is : 1.0\n"
     ]
    }
   ],
   "source": [
    "print('the label of the first entry on our data is :' ,y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, by seeing that the first label of our dataset is \"s\" which stands for signal, **y =1.0 means that there is a signal, and y=-1.0 means background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's see if the data contains NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y has NaN values ? False\n",
      "tX has NaN values ? False\n",
      "ids has NaN values ? False\n",
      "(array([     1,      1,      1, ..., 249999, 249999, 249999], dtype=int64), array([ 4,  5,  6, ..., 26, 27, 28], dtype=int64))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-ef7edc514d89>:3: RuntimeWarning: overflow encountered in long_scalars\n",
      "  print('ids has NaN values ?', np.isnan(sum(ids)))\n"
     ]
    }
   ],
   "source": [
    "print('y has NaN values ?',np.isnan(sum(y)))\n",
    "print('tX has NaN values ?',np.isnan(sum(sum(tX)))) #tX has 2 dimensions so we sum over the two dimensions\n",
    "print('ids has NaN values ?', np.isnan(sum(ids)))\n",
    "\n",
    "print(np.where(tX==-999.0)) #several -999.0 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As decribed on AICrowd, features prefixed with DER are derived from features prefixed with PRI so we can only keep the \"raw\" features, those prefixed with PRI, without losing information. This will also reduce to size of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus there is a lot of \"-999.0\" in our dataset so we must remove them. A firts idea is to replace them by 0. An other idea is to replace these values by the mean or the median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manipulate_data import *\n",
    "\n",
    "tX = clean_data(tX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new shape is : (250000, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the new shape is :',tX.shape)\n",
    "np.where(tX==-999.0) #the array is empty : there is no more -999.0 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms :\n",
    "\n",
    "**1.Least squares with Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 4 5]]\n",
      "[[  1.   1.   1.   1.   2.   3.   1.   4.   9.   1.   8.  27.   1.  16.\n",
      "   81.]\n",
      " [  1.   1.   1.   3.   4.   5.   9.  16.  25.  27.  64. 125.  81. 256.\n",
      "  625.]]\n"
     ]
    }
   ],
   "source": [
    "ttt= np.array([[1,2,3], [3,4,5]])\n",
    "print(ttt)\n",
    "print(build_poly(ttt,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without any polynomial augmentation : degree fixed to 1, gamma varies\n",
      "for gamma = 0.7, loss = 1.175921591050787e+202\n",
      "for gamma = 0.1, loss = 1.8434979074371061e+168\n",
      "for gamma = 0.001, loss = 1.4837295600462847e+88\n",
      "for gamma = 0.0001, loss = 1.9492650062973896e+47\n",
      "for gamma = 1e-05, loss = 0.4008444611570252\n",
      "for gamma = 1e-06, loss = 0.44832327176287684\n",
      "for gamma = 1e-07, loss = 0.4884308641506899\n",
      "for gamma = 1e-08, loss = 0.49866753660288426\n",
      "\n",
      "Gamma fixed : 1e-15, degree varies\n",
      "for degree = 1, loss = 0.4999999998643966\n",
      "for degree = 3, loss = 2.1154128119337214e+118\n",
      "for degree = 5, loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-37a9e0af81b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtX_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mw_lsGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_lsGD\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ML_Projects\\project1\\scripts\\manipulate_data.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mPHI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mPHI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPHI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mPHI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "from manipulate_data import *\n",
    "\n",
    "\n",
    "#Least Squares - GD\n",
    "max_iters = 20 \n",
    "gammas = [0.7, 0.1, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8] #by testing the results: this seems to be the optimal value\n",
    "initial_w = np.zeros((tX.shape[1],)) #number of parameters = number of features of x\n",
    "\n",
    "print('Without any polynomial augmentation : degree fixed to 1, gamma varies')\n",
    "for gamma in gammas:\n",
    "    w_lsGD, loss_lsGD= least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "    print('for gamma = {}, loss = {}'.format(gamma,loss_lsGD))\n",
    "\n",
    "\n",
    "#We do it with polynomial features\n",
    "degrees=[1, 3, 5, 7, 9, 11]\n",
    "gamma = 1e-15\n",
    "print('\\nGamma fixed : {}, degree varies'.format(gamma))\n",
    "\n",
    "for degree in degrees:\n",
    "    tX_exp = build_poly(tX, degree)\n",
    "    initial_w = np.zeros(((degree+1)*tX.shape[1],))\n",
    "    w_lsGD, loss_lsGD= least_squares_GD(y, tX_exp, initial_w, max_iters, gamma)\n",
    "    print('for degree = {}, loss = {}'.format(degree,loss_lsGD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without any polynomial augmentation : degree fixed to 1, gamma varies\n",
      "\n",
      "Gamma fixed : 1e-10, degree varies\n",
      "for degree = 1, loss = 0.4999999821596929\n",
      "for degree = 3, loss = 0.49999949287654283\n",
      "for degree = 5, loss = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\ML_Projects\\project1\\scripts\\implementations.py:79: RuntimeWarning: invalid value encountered in subtract\n",
      "  loss = compute_loss(y, tx, w, lambda_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for degree = 7, loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-fdea4509b37a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtX_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtX_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mw_lsGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_lsGD\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ML_Projects\\project1\\scripts\\manipulate_data.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mPHI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mPHI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPHI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mPHI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#same experiment with tX normalized :\n",
    "tX_norm, mean_tx, std_tx = standardise(tX)\n",
    "\n",
    "max_iters = 50 \n",
    "gammas = [0.7, 0.1, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8] #by testing the results: this seems to be the optimal value\n",
    "initial_w = np.zeros((tX_norm.shape[1],)) #number of parameters = number of features of x\n",
    "\n",
    "print('Without any polynomial augmentation : degree fixed to 1, gamma varies')\n",
    "'''\n",
    "for gamma in gammas:\n",
    "    w_lsGD, loss_lsGD= least_squares_GD(y, tX_norm, initial_w, max_iters, gamma)\n",
    "    print('for gamma = {}, loss = {}'.format(gamma,loss_lsGD))'''\n",
    "\n",
    "\n",
    "    \n",
    "#We do it with polynomial features\n",
    "degrees=[1, 3, 5, 7, 9, 11]\n",
    "gamma = 1e-10\n",
    "print('\\nGamma fixed : {}, degree varies'.format(gamma))\n",
    "\n",
    "for degree in degrees:\n",
    "    tX_exp = build_poly(tX_norm, degree)\n",
    "    initial_w = np.zeros(((degree+1)*tX_norm.shape[1],))\n",
    "    w_lsGD, loss_lsGD= least_squares_GD(y, tX_exp, initial_w, max_iters, gamma)\n",
    "    print('for degree = {}, loss = {}'.format(degree,loss_lsGD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the parameters to have a final value and do prediction\n",
    "max_iters=200\n",
    "gamma=1e-5\n",
    "initial_w = np.zeros((tX_norm.shape[1],))\n",
    "w_lsGD, loss_lsGD= least_squares_GD(y, tX_norm, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4988176737354244\n"
     ]
    }
   ],
   "source": [
    "print(loss_lsGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Least squares with SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without any polynomial augmentation : degree fixed to 1, gamma varies\n",
      "for gamma = 0.7, loss = 5.562278705066963e+18\n",
      "for gamma = 0.1, loss = 4.798443196321069\n",
      "for gamma = 0.001, loss = 0.48901147610967105\n",
      "for gamma = 0.0001, loss = 0.49793485597068954\n",
      "for gamma = 1e-05, loss = 0.4997719114864903\n",
      "for gamma = 1e-06, loss = 0.49998822885630745\n",
      "for gamma = 1e-07, loss = 0.49999956760972925\n",
      "for gamma = 1e-08, loss = 0.4999998937511005\n",
      "\n",
      "Gamma fixed : 1e-10, degree varies\n",
      "for degree = 1, loss = 0.49999999164742137\n",
      "for degree = 3, loss = 0.49999996866101304\n",
      "for degree = 5, loss = 0.5068218543105351\n",
      "for degree = 7, loss = 366316.4245951973\n",
      "for degree = 9, loss = 4.347690980520219e+19\n",
      "for degree = 11, loss = 2.088939160245814e+29\n"
     ]
    }
   ],
   "source": [
    "#Least squares with SGD on normalized data\n",
    "tX_norm, mean_tx, std_tx = standardise(tX)\n",
    "\n",
    "max_iters = 20 \n",
    "gammas = [0.7, 0.1, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8] #by testing the results: this seems to be the optimal value\n",
    "initial_w = np.zeros((tX_norm.shape[1],)) #number of parameters = number of features of x\n",
    "\n",
    "print('Without any polynomial augmentation : degree fixed to 1, gamma varies')\n",
    "for gamma in gammas:\n",
    "    w_lsSGD, loss_lsSGD= least_squares_SGD(y, tX_norm, initial_w, max_iters, gamma)\n",
    "    print('for gamma = {}, loss = {}'.format(gamma,loss_lsSGD))\n",
    "\n",
    "    \n",
    "#We do it with polynomial features\n",
    "degrees=[1, 3, 5, 7, 9, 11]\n",
    "gamma = 1e-10\n",
    "print('\\nGamma fixed : {}, degree varies'.format(gamma))\n",
    "\n",
    "for degree in degrees:\n",
    "    tX_exp = build_poly(tX_norm, degree)\n",
    "    initial_w = np.zeros(((degree+1)*tX_norm.shape[1],))\n",
    "    w_lsSGD, loss_lsSGD= least_squares_SGD(y, tX_exp, initial_w, max_iters, gamma)\n",
    "    print('for degree = {}, loss = {}'.format(degree,loss_lsSGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.3477943467733908\n",
      "weight : [-6.53073663e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03 -6.53087895e-03 -6.53087895e-03\n",
      " -6.53087895e-03 -6.53087895e-03  2.83033873e-02 -4.65351591e-02\n",
      "  1.33752492e-02  1.13086425e-02 -1.41287772e-03  1.50636370e-03\n",
      " -2.24173691e-03  1.30614188e-02 -5.28478255e-03  1.24909309e-02\n",
      " -3.24594084e-02  1.19562605e-02  2.21375398e-03  4.07053360e-02\n",
      " -2.88597912e-04 -2.99449737e-04  8.42267217e-04  2.19747680e-04\n",
      "  3.42420697e-04  7.05618928e-04  6.91918162e-04  6.83295747e-03\n",
      " -2.17331358e-03  7.16212941e-03 -3.47187624e-04 -1.75762923e-04\n",
      "  2.11018456e-03  1.28398347e-04 -9.33008509e-05  5.24815790e-03\n",
      " -3.21095547e-02  6.84662335e-03 -3.39278288e-02  1.13423391e-02\n",
      " -1.75839516e-03  2.89172284e-03  7.70086988e-04 -8.30351250e-03\n",
      " -9.71057743e-04 -4.58686026e-03  1.33213296e-02  2.26194535e-03\n",
      " -8.62190374e-05 -6.18619346e-03 -6.98431428e-03 -4.76982286e-03\n",
      " -4.84866015e-04 -1.01654407e-02 -5.02436392e-03  1.57628165e-02\n",
      " -4.33143660e-03 -8.15762207e-03 -2.63415024e-03  2.37056520e-03\n",
      "  4.50507932e-03 -5.71625309e-03 -2.64172864e-04  3.41929155e-04\n",
      " -2.68684114e-03 -2.48755681e-03  1.19987211e-02 -3.63402165e-02\n",
      " -2.58809734e-03  1.05949724e-02  1.83889125e-03  5.39622476e-03\n",
      " -2.90510239e-03  9.84368595e-03  1.63121108e-03  7.71275197e-03\n",
      " -2.40410165e-02  1.11281787e-02  1.84730105e-03  9.34599459e-03\n",
      " -3.03560851e-04 -1.50518185e-04  3.49037390e-03  3.85496452e-05\n",
      "  5.60028307e-04 -4.64859702e-03  4.37515533e-04  4.79360197e-03\n",
      " -2.17837109e-03  7.48664047e-04 -2.94027328e-04 -1.54672546e-04\n",
      "  1.53556763e-03  1.78063245e-04 -1.14432503e-04 -2.57867272e-03\n",
      " -9.30493085e-03  1.80264385e-02  2.12814391e-03  8.83303460e-03\n",
      "  3.81767297e-03 -1.11580562e-03  9.23891537e-05 -2.32545514e-02\n",
      " -7.89251992e-05 -2.42438359e-03  7.16964206e-03  5.80833373e-03\n",
      "  1.89369721e-03 -2.43488273e-03 -8.25615848e-03 -3.92738888e-03\n",
      " -8.49605734e-04 -1.16235514e-02 -4.22939857e-03  1.85028339e-04\n",
      " -3.32085892e-03 -6.21990139e-03 -2.60394842e-03 -3.88335184e-03\n",
      "  1.37163581e-02 -6.45588812e-03 -2.42766550e-04  2.50320938e-03\n",
      " -3.40255981e-03 -9.36163075e-04  2.08636961e-03 -2.82991480e-03\n",
      " -2.26718313e-04 -4.06522280e-03  4.39182978e-03  5.16037830e-05\n",
      "  9.85198134e-05  5.39275101e-03 -6.46046970e-06  6.63201950e-04\n",
      " -8.22116936e-04  1.18049925e-02  2.23893037e-03  2.20112667e-04\n",
      " -1.30544220e-04 -6.00632729e-05  6.27878951e-05 -1.74753453e-04\n",
      "  5.75931402e-04  6.67545683e-05  3.28450098e-04  1.52214405e-03\n",
      " -1.88769396e-03  1.13912713e-03  2.05505112e-04  1.37374282e-05\n",
      "  8.68900280e-06 -9.76149843e-06 -1.38070861e-04  2.26503375e-04\n",
      " -1.72568403e-04  1.77736834e-04  8.83006871e-06  5.26840989e-04\n",
      " -2.75780546e-03  1.42139366e-06 -4.47527996e-06  2.38759586e-03\n",
      "  4.12519077e-07 -8.10719185e-05  4.03192242e-05  9.40956196e-03\n",
      "  1.07591352e-03 -8.19384905e-06 -1.16410909e-03 -3.78881158e-04\n",
      " -9.97012029e-07 -2.42573165e-03 -6.89014218e-04 -7.34770820e-06\n",
      " -5.37986547e-04 -1.28721936e-04 -1.76618608e-03 -1.17637726e-04\n",
      " -1.01888384e-03  8.09110466e-04  1.55169432e-07 -1.24687894e-04\n",
      "  3.20558495e-04 -1.38049024e-05  4.77928283e-06 -3.88537386e-06\n",
      " -1.16582128e-07 -2.18233692e-05  3.58028852e-04 -1.00759209e-07\n",
      " -5.26848493e-07 -5.69143449e-04 -4.05520105e-09  3.22660912e-06\n",
      " -7.08199829e-07  1.34742761e-02 -5.94317782e-04  1.07756881e-07\n",
      "  1.05577139e-04  6.08529846e-05 -2.89528319e-08  1.32418866e-04\n",
      " -4.44670252e-04  2.05579201e-07 -1.67295132e-04  3.35899212e-06\n",
      "  9.70266159e-05  4.13581187e-06 -5.27788814e-06  4.42473133e-05\n",
      " -9.03089139e-09  8.68079097e-07  1.59715051e-05  2.48588095e-07]\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 1 #doesn't seem to change anything, we almost sure to be overfitting : loss is very very small\n",
    "tX_exp = build_poly(tX_norm, 7)\n",
    "\n",
    "w_rr, loss_rr = ridge_regression(y, tX_exp, lambda_)\n",
    "print('loss :', loss_rr)\n",
    "print('weight :', w_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without any polynomial augmentation : degree fixed to 1, gamma varies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\ML_Projects\\project1\\scripts\\implementations.py:42: RuntimeWarning: overflow encountered in exp\n",
      "  #loss = (1/2) * ( 2*np.sum(np.log(1 + np.exp(z))) - np.dot(y.T, z) - z)\n",
      "C:\\Users\\samsung\\ML_Projects\\project1\\scripts\\implementations.py:34: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t) / (1+np.exp(t))\n",
      "C:\\Users\\samsung\\ML_Projects\\project1\\scripts\\implementations.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(t) / (1+np.exp(t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for gamma = 0.7, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 0.1, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 0.001, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 0.0001, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 1e-05, loss = [190553.00520973 190551.53647459 190553.46601892 ... 190552.23651374\n",
      " 190550.74752928 190551.9057004 ]\n",
      "for gamma = 1e-06, loss = [147438.62105186 147438.40825538 147439.43093211 ... 147438.65685885\n",
      " 147438.30835774 147438.93419434]\n",
      "for gamma = 1e-07, loss = [162227.93825257 162227.95649106 162228.18169502 ... 162228.00538184\n",
      " 162227.97692149 162228.10852326]\n",
      "for gamma = 1e-08, loss = [171537.37795747 171537.38343033 171537.40770947 ... 171537.38870024\n",
      " 171537.38803516 171537.40263179]\n",
      "\n",
      "Gamma fixed : 1e-10, degree varies\n",
      "for degree = 1, loss = [172881.17280499 172881.17286537 172881.17310989 ... 172881.17291848\n",
      " 172881.17291538 172881.17306308]\n",
      "for degree = 3, loss = [171897.61132664 171897.61075894 171897.62192752 ... 171897.61036134\n",
      " 171897.61051191 171897.61041016]\n",
      "for degree = 5, loss = [nan nan nan ... nan nan nan]\n",
      "for degree = 7, loss = [nan nan nan ... nan nan nan]\n",
      "for degree = 9, loss = [nan nan nan ... nan nan nan]\n",
      "for degree = 11, loss = [nan nan nan ... nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression (GD) on normalized data without regularization\n",
    "max_iters = 20 \n",
    "gammas = [0.7, 0.1, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8] #by testing the results: this seems to be the optimal value\n",
    "initial_w = np.zeros((tX_norm.shape[1],)) #number of parameters = number of features of x\n",
    "\n",
    "print('Without any polynomial augmentation : degree fixed to 1, gamma varies')\n",
    "for gamma in gammas:\n",
    "    w_lr, loss_lr= logistic_regression(y, tX_norm, initial_w, max_iters, gamma)\n",
    "    print('for gamma = {}, loss = {}'.format(gamma,loss_lr))\n",
    "\n",
    "    \n",
    "#We do it with polynomial features\n",
    "degrees=[1, 3, 5, 7, 9, 11]\n",
    "gamma = 1e-10\n",
    "print('\\nGamma fixed : {}, degree varies'.format(gamma))\n",
    "\n",
    "for degree in degrees:\n",
    "    tX_exp = build_poly(tX_norm, degree)\n",
    "    initial_w = np.zeros(((degree+1)*tX_norm.shape[1],))\n",
    "    w_lr, loss_lr= logistic_regression(y, tX_exp, initial_w, max_iters, gamma)\n",
    "    print('for degree = {}, loss = {}'.format(degree,loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without any polynomial augmentation : degree fixed to 1, gamma varies\n",
      "for gamma = 0.7, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 0.1, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 0.001, loss = [nan nan nan ... nan nan nan]\n",
      "for gamma = 0.0001, loss = [1116326.89719379 1116316.40389759 1116340.96770706 ... 1116322.28186011\n",
      " 1116310.91208137 1116323.50786989]\n",
      "for gamma = 1e-05, loss = [197523.12122382 197521.66548529 197523.66191656 ... 197522.33019692\n",
      " 197520.89209248 197522.0181163 ]\n",
      "for gamma = 1e-06, loss = [147912.92526239 147912.71723089 147913.72514514 ... 147912.96206826\n",
      " 147912.61979762 147913.23646928]\n",
      "for gamma = 1e-07, loss = [162264.18093992 162264.19916144 162264.42394742 ... 162264.24796144\n",
      " 162264.21956528 162264.35092499]\n",
      "for gamma = 1e-08, loss = [171538.04660189 171538.05207374 171538.07634827 ... 171538.05734265\n",
      " 171538.05667771 171538.07127157]\n",
      "\n",
      "Gamma fixed : 1e-10, degree varies\n",
      "for degree = 1, loss = [172881.1737973  172881.17385768 172881.1741022  ... 172881.17391079\n",
      " 172881.17390769 172881.17405539]\n",
      "for degree = 3, loss = [171897.62776849 171897.6272008  171897.63836936 ... 171897.6268032\n",
      " 171897.62695377 171897.62685202]\n",
      "for degree = 5, loss = [nan nan nan ... nan nan nan]\n",
      "for degree = 7, loss = [nan nan nan ... nan nan nan]\n",
      "for degree = 9, loss = [nan nan nan ... nan nan nan]\n",
      "for degree = 11, loss = [nan nan nan ... nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression (GD) on normalized data with regularization\n",
    "max_iters = 20 \n",
    "gammas = [0.7, 0.1, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8] #by testing the results: this seems to be the optimal value\n",
    "initial_w = np.zeros((tX_norm.shape[1],)) #number of parameters = number of features of x\n",
    "lambda_ = 1000\n",
    "\n",
    "print('Without any polynomial augmentation : degree fixed to 1, gamma varies')\n",
    "for gamma in gammas:\n",
    "    w_lr, loss_lr= reg_logistic_regression(y, tX_norm, lambda_, initial_w, max_iters, gamma)\n",
    "    print('for gamma = {}, loss = {}'.format(gamma,loss_lr))\n",
    "\n",
    "    \n",
    "#We do it with polynomial features\n",
    "degrees=[1, 3, 5, 7, 9, 11]\n",
    "gamma = 1e-10\n",
    "print('\\nGamma fixed : {}, degree varies'.format(gamma))\n",
    "\n",
    "for degree in degrees:\n",
    "    tX_exp = build_poly(tX_norm, degree)\n",
    "    initial_w = np.zeros(((degree+1)*tX_norm.shape[1],))\n",
    "    w_lr, loss_lr= reg_logistic_regression(y, tX_exp, lambda_, initial_w, max_iters, gamma)\n",
    "    print('for degree = {}, loss = {}'.format(degree,loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed paramters to compute optimal weight for logistic regression\n",
    "max_iters = 100 \n",
    "gamma = 1e-06\n",
    "initial_w = np.zeros((tX_norm.shape[1],))\n",
    "lambda_ = 0\n",
    "w_lr, loss_lr= reg_logistic_regression(y, tX_norm, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'C:/Users/samsung/ML_Projects/project1/data/test/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test = clean_data(tX_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on test data for least squares =  0.5299287820655979\n",
      "loss on test data for logistic regression =  3366.901374460843\n"
     ]
    }
   ],
   "source": [
    "#Loss on test data\n",
    "print('loss on test data for least squares = ', compute_reg_ls_loss(_, tX_test, w_lsGD, lambda_=0))\n",
    "print('loss on test data for logistic regression = ', compute_reg_ls_loss(_, tX_test, w_lr, lambda_=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
