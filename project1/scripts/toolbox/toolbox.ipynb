{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 20\n",
    "noise_std = 3.0\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import standardise, split_data\n",
    "\n",
    "def gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std, randomised=True):\n",
    "    x = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    x, mean_x, std_x = standardise(x)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    y = np.dot(x, true_w)\n",
    "    if randomised:\n",
    "        y += np.random.normal(0.0, noise_std, N)\n",
    "    \n",
    "    return y, x, true_w\n",
    "\n",
    "\n",
    "y, x, true_w = gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std, True)\n",
    "y_train, x_train, y_test, x_test = split_data(y, x, train_ratio)\n",
    "\n",
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try regression with regularised/unregularised MSE using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Regularised with GD\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.392, test_loss = 9.246, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.404, test_loss = 9.246, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.496, test_loss = 9.247, err_w = 0.004\n",
      "lam = 0.000: train_loss = 5.211, test_loss = 9.254, err_w = 0.005\n",
      "lam = 0.003: train_loss = 10.714, test_loss = 9.370, err_w = 0.008\n",
      "lam = 0.022: train_loss = 51.532, test_loss = 13.757, err_w = 0.044\n",
      "lam = 0.167: train_loss = 287.620, test_loss = 160.903, err_w = 0.258\n",
      "lam = 1.292: train_loss = 806.838, test_loss = 1212.484, err_w = 0.729\n",
      "lam = 10.000: train_loss = 1056.113, test_loss = 2071.914, err_w = 0.954\n",
      "\n",
      " Regularised with SGD\n",
      "lam = 0.000: train_loss = 7.656, test_loss = 15.225, err_w = 0.053\n",
      "lam = 0.000: train_loss = 6.366, test_loss = 12.791, err_w = 0.040\n",
      "lam = 0.000: train_loss = 7.719, test_loss = 15.937, err_w = 0.055\n",
      "lam = 0.000: train_loss = 9.237, test_loss = 18.509, err_w = 0.064\n",
      "lam = 0.000: train_loss = 6.864, test_loss = 12.394, err_w = 0.038\n",
      "lam = 0.003: train_loss = 16.413, test_loss = 19.376, err_w = 0.067\n",
      "lam = 0.022: train_loss = 59.441, test_loss = 30.357, err_w = 0.094\n",
      "lam = 0.167: train_loss = 336.306, test_loss = 277.473, err_w = 0.343\n",
      "lam = 1.292: train_loss = 1055.277, test_loss = 1497.104, err_w = 0.814\n",
      "lam = 10.000: train_loss = 6147.961, test_loss = 1599.291, err_w = 0.844\n",
      "\n",
      " Unregularised with GD\n",
      "train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "\n",
      " Unregularised with SGD\n",
      "train_loss = 8.942, test_loss = 18.541, err_w = 0.064\n",
      "\n",
      "Regularised with NE\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.392, test_loss = 9.246, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.404, test_loss = 9.246, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.496, test_loss = 9.247, err_w = 0.004\n",
      "lam = 0.000: train_loss = 5.211, test_loss = 9.254, err_w = 0.005\n",
      "lam = 0.003: train_loss = 10.714, test_loss = 9.370, err_w = 0.008\n",
      "lam = 0.022: train_loss = 51.532, test_loss = 13.757, err_w = 0.044\n",
      "lam = 0.167: train_loss = 287.620, test_loss = 160.903, err_w = 0.258\n",
      "lam = 1.292: train_loss = 806.838, test_loss = 1212.484, err_w = 0.729\n",
      "lam = 10.000: train_loss = 1056.113, test_loss = 2071.914, err_w = 0.954\n",
      "\n",
      "Unregularised with NE\n",
      "train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import assess_regressor_mse\n",
    "\n",
    "reg_trainers = {\"Regularised with GD\" : train_reg_mse_GD,\n",
    "                \"Regularised with SGD\" : train_reg_mse_SGD}\n",
    "unreg_trainers = {\"Unregularised with GD\" : train_unreg_mse_GD,\n",
    "                  \"Unregularised with SGD\" : train_unreg_mse_SGD}\n",
    "lambdas = np.logspace(-7, 1, 10)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 50000\n",
    "gamma = 0.05\n",
    "\n",
    "\n",
    "def try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, x_train, lambda_, initial_w, max_iters, gamma, 0)\n",
    "            test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "            \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Unregularised with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, x_train, initial_w, max_iters, gamma, 0)\n",
    "        test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_mse_NE(y_train, x_train, lambda_, 0)\n",
    "        test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised with NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_mse_NE(y_train, x_train, 0)\n",
    "    test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 2\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n",
      "[1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0\n",
      " 0 1 0 1 1 0 1 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import standardise, split_data\n",
    "from implementations import sigmoid\n",
    "\n",
    "def gen_bogus_classification_data(N, D, x_lim, w_lim, randomised=True):\n",
    "    x = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    x, mean_x, std_x = standardise(x)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    p = sigmoid(np.dot(x, true_w))\n",
    "    if randomised:\n",
    "        y = np.where(np.random.rand(N) < p, 1, 0)\n",
    "    else:\n",
    "        y = np.where(0.5 < p, 1, 0)\n",
    "    \n",
    "    return y, x, true_w\n",
    "\n",
    "\n",
    "y, x, true_w = gen_bogus_classification_data(N, D, x_lim, w_lim, True)\n",
    "y_train, x_train, y_test, x_test = split_data(y, x, train_ratio)\n",
    "\n",
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)\n",
    "print(y[::200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classification with regularised/unregularised MSE/logistic using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Regularised Least-Squares with GD\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.003: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.022: train_loss = 0.188, test_loss = 0.430, err_w = 0.905\n",
      "lam = 0.167: train_loss = 0.202, test_loss = 0.471, err_w = 0.924\n",
      "lam = 1.292: train_loss = 0.232, test_loss = 0.502, err_w = 0.972\n",
      "lam = 10.000: train_loss = 0.247, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      " Regularised Least-Squares with SGD\n",
      "lam = 0.000: train_loss = 0.187, test_loss = 0.419, err_w = 0.900\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.419, err_w = 0.900\n",
      "lam = 0.000: train_loss = 0.187, test_loss = 0.406, err_w = 0.893\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.417, err_w = 0.899\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.422, err_w = 0.901\n",
      "lam = 0.003: train_loss = 0.188, test_loss = 0.415, err_w = 0.900\n",
      "lam = 0.022: train_loss = 0.189, test_loss = 0.426, err_w = 0.903\n",
      "lam = 0.167: train_loss = 0.202, test_loss = 0.464, err_w = 0.921\n",
      "lam = 1.292: train_loss = 0.234, test_loss = 0.502, err_w = 0.971\n",
      "lam = 10.000: train_loss = 0.249, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      " Regularised Logistic with GD\n",
      "lam = 0.000: train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.523, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.527, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.003: train_loss = 1904.558, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.022: train_loss = 1904.801, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.167: train_loss = 1906.673, test_loss = 0.154, err_w = 0.047\n",
      "lam = 1.292: train_loss = 1920.850, test_loss = 0.153, err_w = 0.044\n",
      "lam = 10.000: train_loss = 2015.820, test_loss = 0.153, err_w = 0.125\n",
      "\n",
      " Regularised Logistic with SGD\n",
      "lam = 0.000: train_loss = 4405.081, test_loss = 0.387, err_w = 0.919\n",
      "lam = 0.000: train_loss = 4401.471, test_loss = 0.384, err_w = 0.920\n",
      "lam = 0.000: train_loss = 4403.744, test_loss = 0.387, err_w = 0.920\n",
      "lam = 0.000: train_loss = 4399.911, test_loss = 0.388, err_w = 0.919\n",
      "lam = 0.000: train_loss = 4358.224, test_loss = 0.386, err_w = 0.915\n",
      "lam = 0.003: train_loss = 4355.208, test_loss = 0.386, err_w = 0.917\n",
      "lam = 0.022: train_loss = 3933.187, test_loss = 0.368, err_w = 0.882\n",
      "lam = 0.167: train_loss = 3268.972, test_loss = 0.321, err_w = 0.854\n",
      "lam = 1.292: train_loss = 3892.011, test_loss = 0.502, err_w = 0.963\n",
      "lam = 10.000: train_loss = 4121.728, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      " Unregularised Least-Squares with GD\n",
      "train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "\n",
      " Unregularised Least-Squares with SGD\n",
      "train_loss = 0.188, test_loss = 0.416, err_w = 0.901\n",
      "\n",
      " Unregularised Logistic with GD\n",
      "train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "\n",
      " Unregularised Logistic with SGD\n",
      "train_loss = 4410.236, test_loss = 0.387, err_w = 0.922\n",
      "\n",
      "Regularised Least-Squares with NE\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.003: train_loss = 0.186, test_loss = 0.419, err_w = 0.898\n",
      "lam = 0.022: train_loss = 0.188, test_loss = 0.427, err_w = 0.902\n",
      "lam = 0.167: train_loss = 0.202, test_loss = 0.470, err_w = 0.924\n",
      "lam = 1.292: train_loss = 0.232, test_loss = 0.502, err_w = 0.972\n",
      "lam = 10.000: train_loss = 0.247, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      "Unregularised Least-Squares with NE\n",
      "train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import assess_classifier_nhd\n",
    "\n",
    "reg_trainers = {\"Regularised Least-Squares with GD\" : train_reg_mse_GD,\n",
    "                \"Regularised Least-Squares with SGD\" : train_reg_mse_SGD,\n",
    "                \"Regularised Logistic with GD\" : train_reg_log_GD,\n",
    "                \"Regularised Logistic with SGD\" : train_reg_log_SGD,\n",
    "               }\n",
    "unreg_trainers = {\"Unregularised Least-Squares with GD\" : train_unreg_mse_GD,\n",
    "                  \"Unregularised Least-Squares with SGD\" : train_unreg_mse_SGD,\n",
    "                  \"Unregularised Logistic with GD\" : train_unreg_log_GD,\n",
    "                  \"Unregularised Logistic with SGD\" : train_unreg_log_SGD,}\n",
    "lambdas = np.logspace(-7, 1, 10)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 5000\n",
    "gamma = 0.001\n",
    "\n",
    "\n",
    "def try_classifiers(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised Least-Squares/Logistic with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, x_train, lambda_, initial_w, max_iters, gamma, 0.5)\n",
    "            test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "                     \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "                \n",
    "    # Unregularised Least-Squares/Logistic with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, x_train, initial_w, max_iters, gamma, 0.5)\n",
    "        test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_mse_NE(y_train, x_train, lambda_, 0.5)\n",
    "        test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised Least-Squares with NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_mse_NE(y_train, x_train, 0.5)\n",
    "    test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "try_classifiers(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
