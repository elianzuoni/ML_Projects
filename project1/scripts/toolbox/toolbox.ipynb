{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 20\n",
    "noise_std = 3.0\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import standardise, split_data\n",
    "\n",
    "def gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std, randomised=True):\n",
    "    x = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    x, mean_x, std_x = standardise(x)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    y = np.dot(x, true_w)\n",
    "    if randomised:\n",
    "        y += np.random.normal(0.0, noise_std, N)\n",
    "    \n",
    "    return y, x, true_w\n",
    "\n",
    "\n",
    "y, x, true_w = gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std, True)\n",
    "y_train, x_train, y_test, x_test = split_data(y, x, train_ratio)\n",
    "\n",
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try regression with regularised/unregularised MSE using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Regularised Least-Squares with GD\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "\n",
      " Regularised Least-Squares with SGD\n",
      "lam = 0.000: train_loss = 4.425, test_loss = 9.327, err_w = 0.008\n",
      "\n",
      " Unregularised Least-Squares with GD\n",
      "train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "\n",
      " Unregularised Least-Squares with SGD\n",
      "train_loss = 4.416, test_loss = 9.257, err_w = 0.005\n",
      "\n",
      "Regularised Least-Squares with NE\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "\n",
      "Unregularised Least-Squares with NE\n",
      "train_loss = 4.390, test_loss = 9.246, err_w = 0.004\n",
      "CV: (8.98926464283262, 4.496750516737043)\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import assess_regressor_mse, cross_validation\n",
    "\n",
    "lambdas = np.logspace(-7, 1, 1)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 5000\n",
    "batch_size = 32\n",
    "gamma = 0.05\n",
    "reg_trainers = {\"Regularised Least-Squares with GD\" : (train_reg_ls_GD, (initial_w, max_iters, gamma)),\n",
    "                \"Regularised Least-Squares with SGD\" : (train_reg_ls_SGD, (initial_w, max_iters, batch_size, gamma))}\n",
    "unreg_trainers = {\"Unregularised Least-Squares with GD\" : (train_unreg_ls_GD, (initial_w, max_iters, gamma)),\n",
    "                  \"Unregularised Least-Squares with SGD\" : (train_unreg_ls_SGD, (initial_w, max_iters, batch_size, gamma))}\n",
    "\n",
    "\n",
    "def try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer, extra_params = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, x_train, lambda_, *extra_params, 0)\n",
    "            test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "            \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Unregularised with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer, extra_params = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, x_train, *extra_params, 0)\n",
    "        test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_ls_NE(y_train, x_train, lambda_, 0)\n",
    "        test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised Least-Squares with NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_ls_NE(y_train, x_train, 0)\n",
    "    test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)\n",
    "print(\"CV:\", cross_validation(y, x, train_reg_ls_NE, (1e-5,), 0.5, 5, \"regressor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 2\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n",
      "[1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import standardise, split_data\n",
    "from implementations import sigmoid\n",
    "\n",
    "def gen_bogus_classification_data(N, D, x_lim, w_lim, randomised=True):\n",
    "    x = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    x, mean_x, std_x = standardise(x)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    p = sigmoid(np.dot(x, true_w))\n",
    "    if randomised:\n",
    "        y = np.where(np.random.rand(N) < p, 1, 0)\n",
    "    else:\n",
    "        y = np.where(0.5 < p, 1, 0)\n",
    "    \n",
    "    return y, x, true_w\n",
    "\n",
    "\n",
    "y, x, true_w = gen_bogus_classification_data(N, D, x_lim, w_lim, True)\n",
    "y_train, x_train, y_test, x_test = split_data(y, x, train_ratio)\n",
    "\n",
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)\n",
    "print(y[::200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classification with regularised/unregularised MSE/logistic using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Regularised Least-Squares with GD\n",
      "lam = 0.000: train_loss = 0.178, test_loss = 0.412, err_w = 0.924\n",
      "\n",
      " Regularised Least-Squares with SGD\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_reg_ls_SGD() missing 1 required positional argument: 'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-35f0930f137d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mtry_classifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_trainers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munreg_trainers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-35f0930f137d>\u001b[0m in \u001b[0;36mtry_classifiers\u001b[1;34m(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_trainers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massess_classifier_nhd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0merr_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrue_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_reg_ls_SGD() missing 1 required positional argument: 'threshold'"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import assess_classifier_nhd\n",
    "\n",
    "reg_trainers = {\"Regularised Least-Squares with GD\" : train_reg_ls_GD,\n",
    "                \"Regularised Least-Squares with SGD\" : train_reg_ls_SGD,\n",
    "                \"Regularised Logistic with GD\" : train_reg_log_GD,\n",
    "                \"Regularised Logistic with SGD\" : train_reg_log_SGD,\n",
    "               }\n",
    "unreg_trainers = {\"Unregularised Least-Squares with GD\" : train_unreg_ls_GD,\n",
    "                  \"Unregularised Least-Squares with SGD\" : train_unreg_ls_SGD,\n",
    "                  \"Unregularised Logistic with GD\" : train_unreg_log_GD,\n",
    "                  \"Unregularised Logistic with SGD\" : train_unreg_log_SGD,}\n",
    "lambdas = np.logspace(-7, 1, 1)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 5000\n",
    "gamma = 0.001\n",
    "\n",
    "\n",
    "def try_classifiers(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised Least-Squares/Logistic with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, x_train, lambda_, initial_w, max_iters, gamma, 0.5)\n",
    "            test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "                     \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "                \n",
    "    # Unregularised Least-Squares/Logistic with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, x_train, initial_w, max_iters, gamma, 0.5)\n",
    "        test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_mse_NE(y_train, x_train, lambda_, 0.5)\n",
    "        test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised Least-Squares with NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_mse_NE(y_train, x_train, 0.5)\n",
    "    test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "try_classifiers(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of training failed: Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\anaconda3\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\anaconda3\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Anzuoni Elia\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\", line 10, in <module>\n",
      "    from .implementations import *\n",
      "ImportError: attempted relative import with no known parent package\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regularised Least-Squares with NE\n",
      "lam = 0.000: avg_test_loss = 0.402, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.401, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.402, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.402, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.402, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.403, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.402, avg_train_loss = 0.177\n",
      "lam = 0.001: avg_test_loss = 0.404, avg_train_loss = 0.177\n",
      "lam = 0.004: avg_test_loss = 0.404, avg_train_loss = 0.178\n",
      "lam = 0.014: avg_test_loss = 0.408, avg_train_loss = 0.179\n",
      "lam = 0.052: avg_test_loss = 0.426, avg_train_loss = 0.184\n",
      "lam = 0.193: avg_test_loss = 0.466, avg_train_loss = 0.197\n",
      "lam = 0.720: avg_test_loss = 0.496, avg_train_loss = 0.220\n",
      "lam = 2.683: avg_test_loss = 0.497, avg_train_loss = 0.237\n",
      "lam = 10.000: avg_test_loss = 0.497, avg_train_loss = 0.245\n",
      "\n",
      "Regularised Logistic with GD\n",
      "lam = 0.000: avg_test_loss = 0.498, avg_train_loss = 1.919\n",
      "lam = 0.000: avg_test_loss = 0.498, avg_train_loss = 1.919\n",
      "lam = 0.000: avg_test_loss = 0.498, avg_train_loss = 1.919\n",
      "lam = 0.000: avg_test_loss = 0.498, avg_train_loss = 1.919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-72b23e66b117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mcv_classifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-72b23e66b117>\u001b[0m in \u001b[0;36mcv_classifiers\u001b[1;34m(y, x, lambdas, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mhyper_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mavg_test_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_train_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_reg_log_GD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"classifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         print(\"lam = {lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\testing.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, tx, trainer, hyper_params, threshold, k_fold, to_assess)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hyper_params is a tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Get test loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\u001b[0m in \u001b[0;36mtrain_reg_log_GD\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma, threshold)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;34m\"\"\" Training for L2-regularised (and normalised) Logistic loss with Gradient Descent \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mlearn_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_linear_regressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(learn_args, learn, get_regressor, threshold)\u001b[0m\n\u001b[0;32m     36\u001b[0m     Returns the model (w, loss) and the corresponding regressor and classifier. \"\"\"\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Learn the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearn_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Get the regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma, normalise)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;34m\"\"\" Gradient Descent for the L2-regularised logistic cost function. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# Just provide the right loss and gradient functions to the generic implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_reg_logistic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_reg_logistic_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(y, tx, initial_w, max_iters, gamma, lambda_, normalise, compute_loss, compute_gradient)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# Compute the gradient evaluated at the current point w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Update w, and the corresponding loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36mcompute_reg_logistic_gradient\u001b[1;34m(y, tx, w, lambda_, normalise)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_reg_logistic_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;34m\"\"\" Compute the gradient of the L2-regularised logistic loss function, evaluated at point w. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;34m\"\"\" Compute the sigmoid function, evaluated at point t. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Numerical stabilisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpiecewise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpiecewise\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mpiecewise\u001b[1;34m(x, condlist, funclist, *args, **kw)\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcondlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m                 \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcondlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Machine Learning\\Projects\\project1\\scripts\\toolbox\\implementations.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;34m\"\"\" Compute the sigmoid function, evaluated at point t. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Numerical stabilisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpiecewise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import cross_validation\n",
    "\n",
    "lambdas = np.logspace(-7, 1, 15)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 500\n",
    "gamma = 1e-3\n",
    "\n",
    "\n",
    "def cv_classifiers(y, x, lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised Least-Squares with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        hyper_params = (lambda_, )\n",
    "        avg_test_loss, avg_train_loss = cross_validation(y, x, train_reg_ls_NE, hyper_params, 0.5, 5, \"classifier\")\n",
    "\n",
    "        print(\"lam = {lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "        lam=lambda_, avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "\n",
    "    # Regularised Logistic with GD\n",
    "    print(\"\\nRegularised Logistic with GD\")\n",
    "    for lambda_ in lambdas:\n",
    "        hyper_params = (lambda_, initial_w, max_iters, gamma)\n",
    "        avg_test_loss, avg_train_loss = cross_validation(y, x, train_reg_log_GD, hyper_params, 0.5, 5, \"classifier\")\n",
    "\n",
    "        print(\"lam = {lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "        lam=lambda_, avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "cv_classifiers(y, x, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
