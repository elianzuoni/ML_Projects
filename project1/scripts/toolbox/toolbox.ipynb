{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 10\n",
    "noise_std = 3.0\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import split_data\n",
    "\n",
    "def gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std):\n",
    "    tx = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    y = np.dot(tx, true_w) + np.random.normal(0.0, noise_std, N)\n",
    "    \n",
    "    return y, tx, true_w\n",
    "\n",
    "\n",
    "y, tx, true_w = gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std)\n",
    "y_train, tx_train, y_test, tx_test = split_data(y, tx, train_ratio)\n",
    "\n",
    "print(y_train.shape, tx_train.shape, y_test.shape, tx_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try linear regression with regularised/unregularised MSE using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Regularised with GD\n",
      "lam = 0.000: train_loss = 4.396, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.000: train_loss = 4.416, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.000: train_loss = 4.513, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.001: train_loss = 4.962, test_loss = 4.624, err_w = 0.001\n",
      "lam = 0.005: train_loss = 7.047, test_loss = 4.625, err_w = 0.001\n",
      "lam = 0.022: train_loss = 16.720, test_loss = 4.636, err_w = 0.001\n",
      "lam = 0.100: train_loss = 61.551, test_loss = 4.753, err_w = 0.002\n",
      "lam = 0.464: train_loss = 268.217, test_loss = 6.743, err_w = 0.007\n",
      "lam = 2.154: train_loss = 1197.807, test_loss = 45.152, err_w = 0.033\n",
      "lam = 10.000: train_loss = 4958.767, test_loss = 692.135, err_w = 0.135\n",
      "\n",
      " Regularised with SGD\n",
      "lam = 0.000: train_loss = 5.070, test_loss = 5.257, err_w = 0.004\n",
      "lam = 0.000: train_loss = 4.826, test_loss = 4.967, err_w = 0.003\n",
      "lam = 0.000: train_loss = 4.926, test_loss = 5.072, err_w = 0.004\n",
      "lam = 0.001: train_loss = 5.519, test_loss = 5.083, err_w = 0.004\n",
      "lam = 0.005: train_loss = 7.429, test_loss = 4.985, err_w = 0.003\n",
      "lam = 0.022: train_loss = 17.935, test_loss = 5.835, err_w = 0.006\n",
      "lam = 0.100: train_loss = 62.175, test_loss = 5.082, err_w = 0.004\n",
      "lam = 0.464: train_loss = 269.075, test_loss = 8.415, err_w = 0.010\n",
      "lam = 2.154: train_loss = 1202.792, test_loss = 66.818, err_w = 0.040\n",
      "lam = 10.000: train_loss = 5034.831, test_loss = 784.849, err_w = 0.143\n",
      "\n",
      " Unregularised with GD\n",
      "train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "\n",
      " Unregularised with SGD\n",
      "train_loss = 5.077, test_loss = 5.235, err_w = 0.004\n",
      "\n",
      "Regularised with NE\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.000: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.001: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.005: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.022: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.100: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 0.464: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 2.154: train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n",
      "lam = 10.000: train_loss = 4.390, test_loss = 4.624, err_w = 0.001\n",
      "\n",
      "Unregularised wiht NE\n",
      "train_loss = 4.390, test_loss = 4.623, err_w = 0.001\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from implementations import compute_reg_mse_loss\n",
    "\n",
    "reg_trainers = {\"Regularised with GD\" : train_reg_mse_GD,\n",
    "                \"Regularised with SGD\" : train_reg_mse_SGD}\n",
    "unreg_trainers = {\"Unregularised with GD\" : train_unreg_mse_GD,\n",
    "                  \"Unregularised with SGD\" : train_unreg_mse_SGD}\n",
    "lambdas = np.logspace(-5, 1, 10)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 5000\n",
    "gamma = 0.0001\n",
    "\n",
    "\n",
    "def try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, tx_train, lambda_, initial_w, max_iters, gamma, 0)\n",
    "            test_loss = compute_reg_mse_loss(y_test, tx_test, w, 0)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "            \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Unregularised with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, tx_train, initial_w, max_iters, gamma, 0)\n",
    "        test_loss = compute_reg_mse_loss(y_test, tx_test, w, 0)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_mse_NE(y_train, tx_train, lambda_, 0)\n",
    "        test_loss = compute_reg_mse_loss(y_test, tx_test, w, 0)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised wiht NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_mse_NE(y_train, tx_train, 0)\n",
    "    test_loss = compute_reg_mse_loss(y_test, tx_test, w, 0)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "try_regressors(y_train, tx_train, y_test, tx_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
