{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 20\n",
    "noise_std = 3.0\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import standardise, split_data\n",
    "\n",
    "def gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std, randomised=True):\n",
    "    x = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    x, mean_x, std_x = standardise(x)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    y = np.dot(x, true_w)\n",
    "    if randomised:\n",
    "        y += np.random.normal(0.0, noise_std, N)\n",
    "    \n",
    "    return y, x, true_w\n",
    "\n",
    "\n",
    "y, x, true_w = gen_bogus_regression_data(N, D, x_lim, w_lim, noise_std, True)\n",
    "y_train, x_train, y_test, x_test = split_data(y, x, train_ratio)\n",
    "\n",
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try regression with regularised/unregularised MSE using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: 8.984238777476298\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import assess_regressor_mse, cross_validation\n",
    "\n",
    "lambdas = np.logspace(-7, 1, 10)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 50000\n",
    "batch_size = 32\n",
    "gamma = 0.05\n",
    "reg_trainers = {\"Regularised Least-Squares with GD\" : (train_reg_ls_GD, (initial_w, max_iters, gamma)),\n",
    "                \"Regularised Least-Squares with SGD\" : (train_reg_ls_SGD, (initial_w, max_iters, batch_size, gamma))}\n",
    "unreg_trainers = {\"Unregularised Least-Squares with GD\" : (train_unreg_ls_GD, (initial_w, max_iters, gamma)),\n",
    "                  \"Unregularised Least-Squares with SGD\" : (train_unreg_ls_SGD, (initial_w, max_iters, batch_size, gamma))}\n",
    "\n",
    "\n",
    "def try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer, extra_params = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, x_train, lambda_, *extra_params, 0)\n",
    "            test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "            \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Unregularised with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer, extra_params = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, x_train, *extra_params, 0)\n",
    "        test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_ls_NE(y_train, x_train, lambda_, 0)\n",
    "        test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised Least-Squares with NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_ls_NE(y_train, x_train, 0)\n",
    "    test_loss = assess_regressor_mse(y_test, x_test, regressor)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "#try_regressors(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)\n",
    "print(\"CV:\", cross_validation(y, x, train_reg_ls_NE, (1e-5,), 0.5, 5, \"regressor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 10000\n",
    "D = 17\n",
    "x_lim = 20\n",
    "w_lim = 2\n",
    "train_ratio = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (6000, 17) (4000,) (4000, 17)\n",
      "[1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from manipulate_data import standardise, split_data\n",
    "from implementations import sigmoid\n",
    "\n",
    "def gen_bogus_classification_data(N, D, x_lim, w_lim, randomised=True):\n",
    "    x = x_lim * (2 * np.random.rand(N, D) - 1)\n",
    "    x, mean_x, std_x = standardise(x)\n",
    "    true_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "    p = sigmoid(np.dot(x, true_w))\n",
    "    if randomised:\n",
    "        y = np.where(np.random.rand(N) < p, 1, 0)\n",
    "    else:\n",
    "        y = np.where(0.5 < p, 1, 0)\n",
    "    \n",
    "    return y, x, true_w\n",
    "\n",
    "\n",
    "y, x, true_w = gen_bogus_classification_data(N, D, x_lim, w_lim, True)\n",
    "y_train, x_train, y_test, x_test = split_data(y, x, train_ratio)\n",
    "\n",
    "print(y_train.shape, x_train.shape, y_test.shape, x_test.shape)\n",
    "print(y[::200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classification with regularised/unregularised MSE/logistic using GD/SGD/NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Regularised Least-Squares with GD\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.003: train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "lam = 0.022: train_loss = 0.188, test_loss = 0.430, err_w = 0.905\n",
      "lam = 0.167: train_loss = 0.202, test_loss = 0.471, err_w = 0.924\n",
      "lam = 1.292: train_loss = 0.232, test_loss = 0.502, err_w = 0.972\n",
      "lam = 10.000: train_loss = 0.247, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      " Regularised Least-Squares with SGD\n",
      "lam = 0.000: train_loss = 0.187, test_loss = 0.419, err_w = 0.900\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.419, err_w = 0.900\n",
      "lam = 0.000: train_loss = 0.187, test_loss = 0.406, err_w = 0.893\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.417, err_w = 0.899\n",
      "lam = 0.000: train_loss = 0.186, test_loss = 0.422, err_w = 0.901\n",
      "lam = 0.003: train_loss = 0.188, test_loss = 0.415, err_w = 0.900\n",
      "lam = 0.022: train_loss = 0.189, test_loss = 0.426, err_w = 0.903\n",
      "lam = 0.167: train_loss = 0.202, test_loss = 0.464, err_w = 0.921\n",
      "lam = 1.292: train_loss = 0.234, test_loss = 0.502, err_w = 0.971\n",
      "lam = 10.000: train_loss = 0.249, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      " Regularised Logistic with GD\n",
      "lam = 0.000: train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.523, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.000: train_loss = 1904.527, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.003: train_loss = 1904.558, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.022: train_loss = 1904.801, test_loss = 0.154, err_w = 0.048\n",
      "lam = 0.167: train_loss = 1906.673, test_loss = 0.154, err_w = 0.047\n",
      "lam = 1.292: train_loss = 1920.850, test_loss = 0.153, err_w = 0.044\n",
      "lam = 10.000: train_loss = 2015.820, test_loss = 0.153, err_w = 0.125\n",
      "\n",
      " Regularised Logistic with SGD\n",
      "lam = 0.000: train_loss = 4405.081, test_loss = 0.387, err_w = 0.919\n",
      "lam = 0.000: train_loss = 4401.471, test_loss = 0.384, err_w = 0.920\n",
      "lam = 0.000: train_loss = 4403.744, test_loss = 0.387, err_w = 0.920\n",
      "lam = 0.000: train_loss = 4399.911, test_loss = 0.388, err_w = 0.919\n",
      "lam = 0.000: train_loss = 4358.224, test_loss = 0.386, err_w = 0.915\n",
      "lam = 0.003: train_loss = 4355.208, test_loss = 0.386, err_w = 0.917\n",
      "lam = 0.022: train_loss = 3933.187, test_loss = 0.368, err_w = 0.882\n",
      "lam = 0.167: train_loss = 3268.972, test_loss = 0.321, err_w = 0.854\n",
      "lam = 1.292: train_loss = 3892.011, test_loss = 0.502, err_w = 0.963\n",
      "lam = 10.000: train_loss = 4121.728, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      " Unregularised Least-Squares with GD\n",
      "train_loss = 0.186, test_loss = 0.424, err_w = 0.901\n",
      "\n",
      " Unregularised Least-Squares with SGD\n",
      "train_loss = 0.188, test_loss = 0.416, err_w = 0.901\n",
      "\n",
      " Unregularised Logistic with GD\n",
      "train_loss = 1904.522, test_loss = 0.154, err_w = 0.048\n",
      "\n",
      " Unregularised Logistic with SGD\n",
      "train_loss = 4410.236, test_loss = 0.387, err_w = 0.922\n",
      "\n",
      "Regularised Least-Squares with NE\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.000: train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n",
      "lam = 0.003: train_loss = 0.186, test_loss = 0.419, err_w = 0.898\n",
      "lam = 0.022: train_loss = 0.188, test_loss = 0.427, err_w = 0.902\n",
      "lam = 0.167: train_loss = 0.202, test_loss = 0.470, err_w = 0.924\n",
      "lam = 1.292: train_loss = 0.232, test_loss = 0.502, err_w = 0.972\n",
      "lam = 10.000: train_loss = 0.247, test_loss = 0.502, err_w = 0.995\n",
      "\n",
      "Unregularised Least-Squares with NE\n",
      "train_loss = 0.185, test_loss = 0.418, err_w = 0.898\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import assess_classifier_nhd\n",
    "\n",
    "reg_trainers = {\"Regularised Least-Squares with GD\" : train_reg_mse_GD,\n",
    "                \"Regularised Least-Squares with SGD\" : train_reg_mse_SGD,\n",
    "                \"Regularised Logistic with GD\" : train_reg_log_GD,\n",
    "                \"Regularised Logistic with SGD\" : train_reg_log_SGD,\n",
    "               }\n",
    "unreg_trainers = {\"Unregularised Least-Squares with GD\" : train_unreg_mse_GD,\n",
    "                  \"Unregularised Least-Squares with SGD\" : train_unreg_mse_SGD,\n",
    "                  \"Unregularised Logistic with GD\" : train_unreg_log_GD,\n",
    "                  \"Unregularised Logistic with SGD\" : train_unreg_log_SGD,}\n",
    "lambdas = np.logspace(-7, 1, 10)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 5000\n",
    "gamma = 0.001\n",
    "\n",
    "\n",
    "def try_classifiers(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, \n",
    "                   lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised Least-Squares/Logistic with GD/SGD\n",
    "    for name in reg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = reg_trainers[name]\n",
    "        for lambda_ in lambdas:\n",
    "            w, train_loss, regressor, classifier = trainer(y_train, x_train, lambda_, initial_w, max_iters, gamma, 0.5)\n",
    "            test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "            err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "                     \n",
    "            print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "            lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "                \n",
    "    # Unregularised Least-Squares/Logistic with GD/SGD\n",
    "    for name in unreg_trainers:\n",
    "        print(\"\\n\", name)\n",
    "        trainer = unreg_trainers[name]\n",
    "        w, train_loss, regressor, classifier = trainer(y_train, x_train, initial_w, max_iters, gamma, 0.5)\n",
    "        test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    # Regularised with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        w, train_loss, regressor, classifier = train_reg_mse_NE(y_train, x_train, lambda_, 0.5)\n",
    "        test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "        err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "        print(\"lam = {lam:.3f}: train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "        lam=lambda_, train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "\n",
    "    # Unregularised with NE\n",
    "    print(\"\\nUnregularised Least-Squares with NE\")\n",
    "    w, train_loss, regressor, classifier = train_unreg_mse_NE(y_train, x_train, 0.5)\n",
    "    test_loss = assess_classifier_nhd(y_test, x_test, classifier)\n",
    "    err_w = np.linalg.norm(w - true_w) / np.linalg.norm(true_w)\n",
    "\n",
    "    print(\"train_loss = {train_loss:.3f}, test_loss = {test_loss:.3f}, err_w = {err_w:.3f}\".format(\n",
    "    train_loss=train_loss, test_loss=test_loss, err_w=err_w))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "try_classifiers(y_train, x_train, y_test, x_test, true_w, reg_trainers, unreg_trainers, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regularised Least-Squares with NE\n",
      "lam = 0.000: avg_test_loss = 0.402, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.404, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.403, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.404, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.403, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.403, avg_train_loss = 0.177\n",
      "lam = 0.000: avg_test_loss = 0.403, avg_train_loss = 0.177\n",
      "lam = 0.001: avg_test_loss = 0.403, avg_train_loss = 0.177\n",
      "lam = 0.004: avg_test_loss = 0.404, avg_train_loss = 0.178\n",
      "lam = 0.014: avg_test_loss = 0.407, avg_train_loss = 0.179\n",
      "lam = 0.052: avg_test_loss = 0.425, avg_train_loss = 0.184\n",
      "lam = 0.193: avg_test_loss = 0.466, avg_train_loss = 0.197\n",
      "lam = 0.720: avg_test_loss = 0.496, avg_train_loss = 0.220\n",
      "lam = 2.683: avg_test_loss = 0.497, avg_train_loss = 0.237\n",
      "lam = 10.000: avg_test_loss = 0.497, avg_train_loss = 0.245\n",
      "\n",
      "Regularised Logistic with GD\n",
      "lam = 0.000: avg_test_loss = 0.110, avg_train_loss = 1941.593\n",
      "lam = 0.000: avg_test_loss = 0.111, avg_train_loss = 1941.192\n",
      "lam = 0.000: avg_test_loss = 0.112, avg_train_loss = 1941.656\n",
      "lam = 0.000: avg_test_loss = 0.112, avg_train_loss = 1941.147\n",
      "lam = 0.000: avg_test_loss = 0.110, avg_train_loss = 1941.973\n",
      "lam = 0.000: avg_test_loss = 0.111, avg_train_loss = 1941.545\n",
      "lam = 0.000: avg_test_loss = 0.112, avg_train_loss = 1941.710\n",
      "lam = 0.001: avg_test_loss = 0.111, avg_train_loss = 1941.426\n",
      "lam = 0.004: avg_test_loss = 0.111, avg_train_loss = 1941.867\n",
      "lam = 0.014: avg_test_loss = 0.110, avg_train_loss = 1942.363\n",
      "lam = 0.052: avg_test_loss = 0.111, avg_train_loss = 1942.656\n",
      "lam = 0.193: avg_test_loss = 0.110, avg_train_loss = 1946.753\n",
      "lam = 0.720: avg_test_loss = 0.111, avg_train_loss = 1959.102\n",
      "lam = 2.683: avg_test_loss = 0.111, avg_train_loss = 2003.917\n",
      "lam = 10.000: avg_test_loss = 0.113, avg_train_loss = 2140.585\n"
     ]
    }
   ],
   "source": [
    "from training import *\n",
    "from testing import cross_validation\n",
    "\n",
    "lambdas = np.logspace(-7, 1, 15)\n",
    "initial_w = w_lim * (2 * np.random.rand(D) - 1)\n",
    "max_iters = 5000\n",
    "gamma = 1e-3\n",
    "\n",
    "\n",
    "def cv_classifiers(y, x, lambdas, initial_w, max_iters, gamma):    \n",
    "    # Regularised Least-Squares with NE\n",
    "    print(\"\\nRegularised Least-Squares with NE\")\n",
    "    for lambda_ in lambdas:\n",
    "        hyper_params = (lambda_, )\n",
    "        avg_test_loss, avg_train_loss = cross_validation(y, x, train_reg_ls_NE, hyper_params, 0.5, 5, \"classifier\")\n",
    "\n",
    "        print(\"lam = {lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "        lam=lambda_, avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "\n",
    "    # Regularised Logistic with GD\n",
    "    print(\"\\nRegularised Logistic with GD\")\n",
    "    for lambda_ in lambdas:\n",
    "        hyper_params = (lambda_, initial_w, max_iters, gamma)\n",
    "        avg_test_loss, avg_train_loss = cross_validation(y, x, train_reg_log_GD, hyper_params, 0.5, 5, \"classifier\")\n",
    "\n",
    "        print(\"lam = {lam:.3f}: avg_test_loss = {avg_test_loss:.3f}, avg_train_loss = {avg_train_loss:.3f}\".format(\n",
    "        lam=lambda_, avg_test_loss=avg_test_loss, avg_train_loss=avg_train_loss))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "cv_classifiers(y, x, lambdas, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
